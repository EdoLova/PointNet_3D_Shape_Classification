# -*- coding: utf-8 -*-
"""tp_soluzione_gcolab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LM2gJC0JVbxg3S6fB9dd34zJkxAaiUfj
"""

!python prepare_data.py

import numpy as np
import matplotlib.pyplot as plt

# Funzione per visualizzare un point cloud
def visualize(x):
    fig = plt.figure(figsize=(10, 7))
    ax = plt.axes(projection="3d")
    ax.scatter3D(x[:,0], x[:,1], x[:,2], color="blue", s=5)
    plt.title("Point Set")
    plt.show()

# Carica un file dal dataset (ad esempio un cilindro)
x = np.loadtxt("data/train/00/000.asc")
visualize(x)

visualize(np.loadtxt("data/train/01/000.asc"))  # parallelepipedo
visualize(np.loadtxt("data/train/02/000.asc"))  # toro



import numpy as np
import torch
from torch import nn
from torch import optim
import torch.nn.functional as F
#from torch.autograd import Variable
from torch.utils.data import DataLoader
from torch.utils import data
from PIL import Image
import torchvision.transforms as transforms
from matplotlib import pyplot as plt
from os import listdir
from os import makedirs
from os.path import join
from os.path import exists
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay


device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def visualize(x):
    # Creating figure
    fig = plt.figure(figsize = (10, 7))
    ax = plt.axes(projection ="3d")

    # Creating plot
    ax.scatter3D(x[:,0], x[:,1], x[:,2], color = "blue")
    plt.title("Point Set")

    # show plot
    plt.show()


class DatasetFromFolder(data.Dataset):
    def __init__(self, datadir):
        super(DatasetFromFolder, self).__init__()

        datadir1 = join(datadir,'00')
        filenames1 = [join(datadir1, x) for x in listdir(datadir1)]

        datadir2 = join(datadir,'01')
        filenames2 = [join(datadir2, x) for x in listdir(datadir2)]

        datadir3 = join(datadir,'02')
        filenames3 = [join(datadir3, x) for x in listdir(datadir3)]

        self.filenames = filenames1 + filenames2 + filenames3

    def __getitem__(self,index):
        name = self.filenames[index]
        input = torch.from_numpy(np.loadtxt(name).transpose(1,0)).type(torch.FloatTensor)

        target = torch.zeros([1], dtype=torch.long)
        if name.find("/00/") >= 0:
            target[0] = 0 #cylinder
        elif name.find("/01/") >=0:
            target[0] = 1 #rectangle
        elif name.find("/02/") >=0:
            target[0] = 2 #torus
        else :
            print("bug")

        return input, target

    def __len__(self):
        return len(self.filenames)


#transform
class MyTNet(nn.Module):
    def __init__(self, dim=3):
        super(MyTNet, self).__init__()
        self.dim = dim

        #TODO
        # COMMENTO: layers convoluzionali 1D
        self.conv1 = nn.Conv1d(dim, 64, 1)
        self.conv2 = nn.Conv1d(64, 128, 1)
        self.conv3 = nn.Conv1d(128, 1024, 1)
        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(1024)

        # COMMENTO: fully connected layers
        self.fc1 = nn.Linear(1024, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, dim * dim)
        self.bn4 = nn.BatchNorm1d(512)
        self.bn5 = nn.BatchNorm1d(256)


    def forward(self,x):

        #TODO
        batch_size = x.size(0)
        # Convoluzioni + ReLU
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))

        # Max pooling globale per aggregare features
        x = torch.max(x, 2)[0]

        # Fully connected + ReLU
        x = F.relu(self.bn4(self.fc1(x)))
        x = F.relu(self.bn5(self.fc2(x)))
        x = self.fc3(x)

        #adding to the identity matrix
        myidentity = torch.from_numpy(np.eye(self.dim, dtype=np.float32)).view(1, self.dim*self.dim).repeat(x.size()[0],1)
        if x.is_cuda:
            myidentity = myidentity.cuda()
        x = x + myidentity
        x = x.view(-1, self.dim, self.dim)

        return x


#PointNet
class MyPointNet(nn.Module):
    def __init__(self, dim=3, dimfeat=64, num_class = 3):
        super(MyPointNet, self).__init__()
        #TODO
        self.dim = dim
        self.dimfeat = dimfeat

        # TNet per input alignment
        self.input_tnet = MyTNet(dim)

        # Convoluzioni iniziali
        self.conv1 = nn.Conv1d(dim, 64, 1)
        self.bn1 = nn.BatchNorm1d(64)

        # TNet per feature alignment
        self.feature_tnet = MyTNet(64)

        # Altre convoluzioni
        self.conv2 = nn.Conv1d(64, 128, 1)
        self.bn2 = nn.BatchNorm1d(128)
        self.conv3 = nn.Conv1d(128, 1024, 1)
        self.bn3 = nn.BatchNorm1d(1024)

        # Fully connected finale
        self.fc1 = nn.Linear(1024, 512)
        self.bn4 = nn.BatchNorm1d(512)
        self.fc2 = nn.Linear(512, 256)
        self.bn5 = nn.BatchNorm1d(256)
        self.fc3 = nn.Linear(256, num_class)

    def forward(self,x):
        #TODO
        # Input TNet
        trans = self.input_tnet(x)
        x = torch.bmm(trans, x)

        # Conv + ReLU
        x = F.relu(self.bn1(self.conv1(x)))

        # Feature TNet
        trans_feat = self.feature_tnet(x)
        x = torch.bmm(trans_feat, x)

        # Conv + ReLU
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))

        # Max pooling globale
        x = torch.max(x, 2)[0]

        # Fully connected
        x = F.relu(self.bn4(self.fc1(x)))
        x = F.relu(self.bn5(self.fc2(x)))
        x = self.fc3(x)

        # Log Softmax
        x = F.log_softmax(x, dim=1)
        return x



myptnet = MyPointNet()

if not exists('mypointnet.pt'):
    myptnet.to(device)

    num_epochs = 100
    num_w = 2
    batch_s = 32

    #Loss and optimizer
    optimizer = optim.SGD(myptnet.parameters(), lr=0.001, momentum=0.9)


    #loading data
    trainset = DatasetFromFolder("data/train")
    trainloader = DataLoader(trainset, num_workers=num_w, batch_size=batch_s, shuffle=True)
    losslog = []


    #train
    for epoch in range(0, num_epochs):
        running_loss = 0.0  # somma delle loss per batch
        for i, data in enumerate(trainloader, 0):
            inputs, targets = data
            inputs = inputs.to(device)
            targets = targets.view(-1).to(device)

            optimizer.zero_grad()
            outputs = myptnet(inputs)
            loss = F.nll_loss(outputs, targets)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()  # accumula la loss del batch

        epoch_loss = running_loss / len(trainloader)  # media loss dell'epoch
        losslog.append(epoch_loss)  # salva nella lista
        #if (epoch-1)%1 == 0 :
        print(f"{epoch} Epoch - training loss:{losslog[-1]:.4f}")

    # display the result
    plt.figure(figsize=(6,4))
    plt.yscale('log')
    plt.plot(losslog, label = 'loss ({:.4f})'.format(losslog[-1]))
    plt.xlabel("Epochs")
    plt.legend()
    plt.show()
    plt.close()

    torch.save(myptnet.state_dict(), 'mypointnet.pt')

else :
    #read the saved model
    myptnet.load_state_dict(torch.load('mypointnet.pt'))

myptnet.eval() #MODIFICA IMPORTANTE: mette BatchNorm/Dropout in eval mode

testset = DatasetFromFolder("data/test")
testloader = DataLoader(testset, num_workers=2, batch_size=1, shuffle=False)

gtlabels = []
predlabels = []

# <-- MODIFICA: disabilita calcolo gradienti durante test
with torch.no_grad():
    for i, data in enumerate(testloader, 0):
        #TODO
        inputs, targets = data
        inputs = inputs.to(device)
        outputs = myptnet(inputs)
        pred = torch.argmax(outputs, dim=1)
        gtlabels.append(targets.item())
        predlabels.append(pred.item())


cm = confusion_matrix(gtlabels, predlabels)
ConfusionMatrixDisplay(cm).plot()

# ===========================
# METRICHE DI CLASSIFICAZIONE
# ===========================
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Calcolo delle metriche globali
accuracy = accuracy_score(gtlabels, predlabels)
precision = precision_score(gtlabels, predlabels, average=None)  # per classe
recall = recall_score(gtlabels, predlabels, average=None)
f1 = f1_score(gtlabels, predlabels, average=None)

# Calcolo delle metriche globali (macro-average)
precision_macro = precision_score(gtlabels, predlabels, average='macro')
recall_macro = recall_score(gtlabels, predlabels, average='macro')
f1_macro = f1_score(gtlabels, predlabels, average='macro')

# ===========================
# STAMPA DEI RISULTATI
# ===========================
print("\n=== METRICHE PER CLASSE ===")
for i in range(len(precision)):
    print(f"Classe {i}:  Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 = {f1[i]:.4f}")

print("\n=== METRICHE GLOBALI ===")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision (macro): {precision_macro:.4f}")
print(f"Recall (macro): {recall_macro:.4f}")
print(f"F1-score (macro): {f1_macro:.4f}")



# punto 5)

#(A) Testare robustezza con rumore

#Aggiungiamo rumore gaussiano ai punti del test set prima di passarli al modello.

# =======================
# (A) TEST CON RUMORE
# =======================

def add_noise(points, noise_level=0.02):
    """
    Aggiunge rumore gaussiano ai punti 3D
    points: tensor (B, 3, N)
    """
    noise = torch.randn_like(points) * noise_level
    return points + noise

#Ora puoi riusare la parte di test, ma passando i dati perturbati:
gtlabels_noise = []
predlabels_noise = []

with torch.no_grad():
    for i, data in enumerate(testloader, 0):
        inputs, targets = data
        inputs = inputs.to(device)

        # Aggiungi rumore ai punti
        noisy_inputs = add_noise(inputs, noise_level=0.02) #Prova a cambiare noise_level in 0.01, 0.05, 0.1 per vedere come cambia l’accuratezza.

        # Passaggio nel modello
        outputs = myptnet(noisy_inputs)
        pred = torch.argmax(outputs, dim=1)

        gtlabels_noise.append(targets.item())
        predlabels_noise.append(pred.item())

# Confusion matrix con rumore
cm_noise = confusion_matrix(gtlabels_noise, predlabels_noise)
ConfusionMatrixDisplay(cm_noise).plot()
plt.title("Confusion Matrix with Gaussian Noise (σ=0.02)")
plt.show()

# ===========================
# METRICHE DI CLASSIFICAZIONE
# ===========================
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Calcolo delle metriche globali
accuracy = accuracy_score(gtlabels, predlabels)
precision = precision_score(gtlabels, predlabels, average=None)  # per classe
recall = recall_score(gtlabels, predlabels, average=None)
f1 = f1_score(gtlabels, predlabels, average=None)

# Calcolo delle metriche globali (macro-average)
precision_macro = precision_score(gtlabels, predlabels, average='macro')
recall_macro = recall_score(gtlabels, predlabels, average='macro')
f1_macro = f1_score(gtlabels, predlabels, average='macro')

# ===========================
# STAMPA DEI RISULTATI
# ===========================
print("\n=== METRICHE PER CLASSE ===")
for i in range(len(precision)):
    print(f"Classe {i}:  Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 = {f1[i]:.4f}")

print("\n=== METRICHE GLOBALI ===")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision (macro): {precision_macro:.4f}")
print(f"Recall (macro): {recall_macro:.4f}")
print(f"F1-score (macro): {f1_macro:.4f}")



#PARTE (B): Training con Data Augmentation (rumore)
# Cella 1 — Funzione per aggiungere rumore + Dataset aggiornato

# =======================
# DATASET CON DATA AUGMENTATION (rumore)
# =======================

import torch
from torch.utils import data
from os.path import join
from os import listdir
import numpy as np

def add_noise(points, noise_level=0.02):
    """
    Aggiunge rumore gaussiano ai punti 3D.
    points: tensor (3, N)
    noise_level: deviazione standard del rumore
    """
    noise = torch.randn_like(points) * noise_level
    return points + noise


class DatasetFromFolder(data.Dataset):
    def __init__(self, datadir, augment=False, noise_level=0.02):
        super(DatasetFromFolder, self).__init__()
        self.augment = augment
        self.noise_level = noise_level

        datadir1 = join(datadir,'00')
        filenames1 = [join(datadir1, x) for x in listdir(datadir1)]

        datadir2 = join(datadir,'01')
        filenames2 = [join(datadir2, x) for x in listdir(datadir2)]

        datadir3 = join(datadir,'02')
        filenames3 = [join(datadir3, x) for x in listdir(datadir3)]

        self.filenames = filenames1 + filenames2 + filenames3

    def __getitem__(self, index):
        name = self.filenames[index]
        input = torch.from_numpy(np.loadtxt(name).transpose(1,0)).type(torch.FloatTensor)

        # Aggiungi rumore solo se augment=True
        if self.augment:
            input = add_noise(input, self.noise_level)

        target = torch.zeros([1], dtype=torch.long)
        if "/00/" in name:
            target[0] = 0
        elif "/01/" in name:
            target[0] = 1
        elif "/02/" in name:
            target[0] = 2
        else:
            print("bug")

        return input, target

    def __len__(self):
        return len(self.filenames)

# Cella 2 — Training con data augmentation

# =======================
# TRAINING CON DATA AUGMENTATION
# =======================

myptnet = MyPointNet()
myptnet.to(device)

num_epochs = 100
num_w = 2
batch_s = 32

# Ottimizzatore
optimizer = optim.SGD(myptnet.parameters(), lr=0.001, momentum=0.9)

# Dataset e dataloader con data augmentation (rumore)
trainset = DatasetFromFolder("data/train", augment=True, noise_level=0.02) #training con data augmentation
trainloader = DataLoader(trainset, num_workers=num_w, batch_size=batch_s, shuffle=True)

losslog = []

print("Training PointNet con data augmentation (rumore σ=0.02)...")

for epoch in range(num_epochs):
    running_loss = 0.0
    myptnet.train()
    for i, data in enumerate(trainloader, 0):
        inputs, targets = data
        inputs = inputs.to(device)
        targets = targets.view(-1).to(device)

        optimizer.zero_grad()
        outputs = myptnet(inputs)
        loss = F.nll_loss(outputs, targets)
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    epoch_loss = running_loss / len(trainloader)
    losslog.append(epoch_loss)
    print(f"{epoch+1}/{num_epochs} Epoch - training loss: {epoch_loss:.4f}")

# Plot curva di loss
plt.figure(figsize=(6,4))
plt.yscale('log')
plt.plot(losslog, label=f'Loss finale ({losslog[-1]:.4f})')
plt.xlabel("Epochs")
plt.ylabel("Training Loss")
plt.legend()
plt.title("PointNet con Data Augmentation (rumore)")
plt.show()

# Salva il modello
torch.save(myptnet.state_dict(), 'mypointnet_augmented.pt')
print("✅ Modello salvato come mypointnet_augmented.pt")

#Cella 3 — Test del modello allenato con data augmentation

#Questa parte serve a valutare quanto il modello migliorato resiste al rumore.
#(È identica alla tua parte di test, ma carica mypointnet_augmented.pt.)

# =======================
# TEST DEL MODELLO ALLENATO CON RUMORE
# =======================

# Carica il modello addestrato con rumore
myptnet.load_state_dict(torch.load('mypointnet_augmented.pt'))
myptnet.eval()

testset = DatasetFromFolder("data/test", augment=False)
testloader = DataLoader(testset, num_workers=2, batch_size=1, shuffle=False)

gtlabels = []
predlabels = []

with torch.no_grad():
    for i, data in enumerate(testloader, 0):
        inputs, targets = data
        inputs = inputs.to(device)
        outputs = myptnet(inputs)
        pred = torch.argmax(outputs, dim=1)
        gtlabels.append(targets.item())
        predlabels.append(pred.item())

cm = confusion_matrix(gtlabels, predlabels)
ConfusionMatrixDisplay(cm).plot()
plt.title("Confusion Matrix - Model Trained with Noise (Data Augmentation)")
plt.show()

# ===========================
# METRICHE DI CLASSIFICAZIONE
# ===========================
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Calcolo delle metriche globali
accuracy = accuracy_score(gtlabels, predlabels)
precision = precision_score(gtlabels, predlabels, average=None)  # per classe
recall = recall_score(gtlabels, predlabels, average=None)
f1 = f1_score(gtlabels, predlabels, average=None)

# Calcolo delle metriche globali (macro-average)
precision_macro = precision_score(gtlabels, predlabels, average='macro')
recall_macro = recall_score(gtlabels, predlabels, average='macro')
f1_macro = f1_score(gtlabels, predlabels, average='macro')

# ===========================
# STAMPA DEI RISULTATI
# ===========================
print("\n=== METRICHE PER CLASSE ===")
for i in range(len(precision)):
    print(f"Classe {i}:  Precision = {precision[i]:.4f}, Recall = {recall[i]:.4f}, F1 = {f1[i]:.4f}")

print("\n=== METRICHE GLOBALI ===")
print(f"Accuracy: {accuracy:.4f}")
print(f"Precision (macro): {precision_macro:.4f}")
print(f"Recall (macro): {recall_macro:.4f}")
print(f"F1-score (macro): {f1_macro:.4f}")

